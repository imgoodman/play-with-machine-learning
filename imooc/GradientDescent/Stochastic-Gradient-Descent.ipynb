{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机梯度下降法\n",
    "\n",
    "学习率随着训练的进行，越来越小\n",
    "\n",
    "最简单的学习率设置：1/迭代次数。\n",
    "缺点：当初始迭代次数很少（例如：1,2等）的时候，学习率的下降变化太大。\n",
    "解决方案是：\n",
    "1. 1/（迭代次数+常数值）\n",
    "2. a/(迭代次数+常数值)\n",
    "3. 模拟退火思想：t0/(迭代次数+t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=100000\n",
    "\n",
    "x=np.random.normal(size=m)\n",
    "X=x.reshape((-1,1))\n",
    "y=4.0*x+3.0+np.random.normal(0.0,3.0, size=m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 先用batch gradient descent实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def J(theta, X_b, y):\n",
    "    return np.sum((X_b.dot(theta) - y)**2)/len(y)\n",
    "\n",
    "def dJ(theta, X_b, y):\n",
    "    return X_b.T.dot(X_b.dot(theta) - y)*2/len(y)\n",
    "\n",
    "def gradient_descent(X_b, y, initial_theta, eta, n_iters=1e4, epsilon=1e-8):\n",
    "    theta=initial_theta\n",
    "    i_iter=0\n",
    "    \n",
    "    while i_iter<n_iters:\n",
    "        gradient=dJ(theta, X_b, y)\n",
    "        last_theta=theta\n",
    "        theta=theta-eta*gradient\n",
    "        \n",
    "        if abs(J(theta, X_b, y) - J(last_theta, X_b ,y))<epsilon:\n",
    "            break\n",
    "        i_iter+=1\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_b=np.hstack([np.ones((len(X),1)), X])\n",
    "initial_theta=np.zeros((X_b.shape[1]))\n",
    "eta=0.01\n",
    "theta=gradient_descent(X_b, y, initial_theta, eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.00482694,  4.00016777])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 再用随机梯度下降法实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dJ_sgd(theta, X_b_i, y_i):\n",
    "    return X_b_i.T.dot(X_b_i.dot(theta) - y_i)*2.0\n",
    "\n",
    "def sgd(X_b, y, initial_theta, n_iters):\n",
    "    t0=5.0\n",
    "    t1=50.0\n",
    "    def learning_rate(t):\n",
    "        return t0/(t+t1)\n",
    "    \n",
    "    theta=initial_theta\n",
    "    i_iter=0\n",
    "    \n",
    "    while i_iter<n_iters:\n",
    "        rand_i=np.random.randint(0, len(X_b))\n",
    "        gradient=dJ_sgd(theta, X_b[rand_i], y[rand_i])\n",
    "        theta=theta - learning_rate(i_iter)*gradient\n",
    "        \n",
    "        i_iter+=1\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 274 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "initial_theta_sgd=np.zeros(X_b.shape[1])\n",
    "theta_sgd=sgd(X_b, y, initial_theta_sgd, len(X_b)//3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.96630065,  3.9987102 ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_sgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可以看出\n",
    "1. 随机梯度下降法，使用更少的迭代次数，也能达到差不多的效果\n",
    "2. 耗费的时间明显减少"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston=load_boston()\n",
    "\n",
    "x=boston.data\n",
    "y=boston.target\n",
    "\n",
    "X=x[y<50]\n",
    "y=y[y<50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardScaler=StandardScaler()\n",
    "standardScaler.fit(X_train)\n",
    "X_train_standard=standardScaler.transform(X_train)\n",
    "X_test_standard=standardScaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.842874971215\n",
      "0.842874971215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\users\\user\\anaconda3\\envs\\keras\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:84: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDRegressor'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "lin_sgd=SGDRegressor()\n",
    "lin_sgd.fit(X_train_standard, y_train)\n",
    "y_test_predict=lin_sgd.predict(X_test_standard)\n",
    "print(r2_score(y_test, y_test_predict))\n",
    "print(lin_sgd.score(X_test_standard, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
